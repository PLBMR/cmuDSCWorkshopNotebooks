{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carnegie Mellon Data Science Club : Practical Natural Language Processing\n",
    "\n",
    "_By [Michael Rosenberg](mailto:mmrosenb@andrew.cmu.edu)._\n",
    "\n",
    "_**Description**: This notebook contains an introduction to document analysis with OkCupid data. It is designed to be used at a workshop for introducing individuals to natural language processing._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Note: If you are using this notebook locally, you can likely skip this step. This step is primarily intended for individuals who want to skip installing all features and want to immediately work on this notebook in the cloud.)_\n",
    "\n",
    "Before we get started, we have a couple things to set up in case you would like to follow along without many installations.\n",
    "\n",
    "1. I am using the **IBM Data Science Experience**, a platform used for collaborating on data analyses across an organization. We can visit the data science experience here: http://datascience.ibm.com\n",
    "\n",
    "2. Set up an account by clicking \"Sign Up\" and moving through the workflow.\n",
    "\n",
    "3. Once you are logged in, click the **Object Storage** section to provide available containers for loading in data assets.\n",
    "\n",
    "4. Go to **My Projects** and click on the **create project** section to initialize a new project. Give your project a name and a description. We will be using ```python``` 2 for this workshop. The ```spark``` version does not really matter, since we will not be using ```spark``` actively. Give your data asset container a name.\n",
    "\n",
    "5. You will now be within your new project dashboard. You can start a new notebook by clicking the **add notebook** button. We will be using ```python``` 2 for this workshop, as discussed before.\n",
    "\n",
    "Also, to clear up potential warnings that we aren't too concerned with in this context, run the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: What Is Natural Language Processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Natural Language Processing\", or NLP for the sake of having an acronym, is the combination of two subphrases. \n",
    "\n",
    "* \"Natural Language\" refers to a system of communication that has been crafted over the centuries by cultures, faiths, nations, and empires. This term is meant to differentiate it from a language that has been artificially crafted by one community (for example, ```python```).\n",
    "\n",
    "* \"Language Processing\" refers to the methods and technologies developed to encode language, edit language, and interpret language. In artificial languages such as ```Java```, processing the language can be translating a script into bytecode for compilation. In natural languages, this could be a translation method such as the Rosetta Stone.\n",
    "\n",
    "Together, \"Natural Language Processing\" refers to the methods and technologies used to infer properties and components of languages in the spoken and written word. Some of these methods include:\n",
    "\n",
    "* Translating a sentence from English to French.\n",
    "\n",
    "* Having a personal assistant respond to your spoken phrases.\n",
    "\n",
    "* Having an artificial intelligence find the logical implications of a written statement found to be true.\n",
    "\n",
    "Today, we will be working on a simpler method: using the language of a writer to predict the age of that writer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note for 15-112 Students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Fundamentals of Programming and Computer Science (15-112), you are taught a way of writing ```python``` (and code in general) for the purpose of writing software. In industry, there are many reasons for writing code, and you may come across instances where you are not writing code specifically meant for the next iPhone app.\n",
    "\n",
    "In this notebook, I will show you the ways of structuring code in a way that is designed for data analysis and model selection. Thus, there will be some coding practices that are somewhat different - and potentially discourage - in the 15-112 curriculum. In particular, the use of global variables is considered rather standard in the practice of data mining.\n",
    "\n",
    "I would like you to be aware that some of these practices in this notebook may need adaptation before leveraged in your 15-112 projects. Most of all, remember to stay within style rules of a course when practicing data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"metadataAnalysis\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before beginning to explore a dataset for prediction and analysis, it is essential that you study how the data is generated in order to inform your exploration.\n",
    "\n",
    "Our dataset is a set of dating profiles from [OkCupid](https://www.okcupid.com), a dating website that targets young adults in a way that parallels dating websites such as [Match.com](match.com) and [eHarmony](eharmony.com). The data was transcribed from a web scrape and presented [as a dataset designed for introductory statistics and data science courses](https://github.com/rudeboybert/JSE_OkCupid).\n",
    "\n",
    "This dataset has many features, but according to the [documentation](data/JSE_OkCupid/okcupid_codebook.txt), the ```essay``` variables contain all language data inputted by users of OkCupid and the ```age``` variable contains the age of the user. For the sake of simplification in our analysis, we will limit our analysis the **self-summary** variable (```essay0```) and the **age** variable (```age```).\n",
    "\n",
    "Things that we should note:\n",
    "\n",
    "* This dataset comes from web scrape, and while transcribed to some degree into a data frame by the data collectors, it is likely to be filled with many assets from the web.\n",
    "\n",
    "* This dataset is primarily user-inputted. This may mean that we will see spelling mistakes related to human error, and that we will see a vocabulary created by the users rather than by the platform owner (OkCupid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scanning a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by loading in the dataset of profiles. This is a ```.csv``` file, which stands for Comma-Separated Values. If we take a look at the [text representation of the dataset](data/JSE_OkCupid/profiles.csv), we see that there is a set of column keys in the first row of the ```.csv``` file, and each row below it refers to a filled-in observation of the dataset. In this context, a \"filled-in observation\" is a transcribed OkCupid profile.\n",
    "\n",
    "Typically, we can load in a ```.csv``` file using the ```csv``` package available in base ```Python```. However, for the sake of having a more elegant coding process, I generally use the ```pandas``` package to manipulative large dataframes. You can refer to the [reference materials](#refMaterials) for instructions on how to install ```pandas```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "okCupidFrame = pd.read_csv(\"data/JSE_OkCupid/profiles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at the dimension of this data frame. This is held in the ```shape``` attribute of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 59946\n",
      "Number of Columns: 31\n"
     ]
    }
   ],
   "source": [
    "numRows = okCupidFrame.shape[0]\n",
    "numCols = okCupidFrame.shape[1]\n",
    "print \"Number of Rows:\", numRows\n",
    "print \"Number of Columns:\", numCols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "numCols": "31",
     "numRows": "59946"
    }
   },
   "source": [
    "We see that there are 59946 profile observations in this dataset, which is a sizable amount of profiles to consider. We also see that each profile contains 31 features, many of which were transcribed by the original data collectors. As discussed in the [metadata analysis](#metadataAnalysis), the language-oriented features are found in the ```essay``` variables. For now, let us consider the self summary variable of the profiles contained in the ```essay0``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selfSummaries = okCupidFrame[\"essay0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first check to see if there are any missing values in this column. This will be important for when we want to use these summaries for predictive purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of entries with null self-summaries is 5485\n"
     ]
    }
   ],
   "source": [
    "#make conditional on which summaries are empty\n",
    "emptySections = selfSummaries[selfSummaries.isnull()]\n",
    "numNullEntries = emptySections.shape[0]\n",
    "print \"The number of entries with null self-summaries is\", numNullEntries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "numNullEntries": "5485"
    }
   },
   "source": [
    "We see that we have 5485 profiles without self-summaries. For the sake of considering only completed profiles up to the summary, we will filter out observations with ```NaN``` entries for ```essay0```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get observations with non-null summaries\n",
    "filteredOkCupidFrame = okCupidFrame[okCupidFrame[\"essay0\"].notnull()]\n",
    "#then reobtain self summaries\n",
    "selfSummaries = filteredOkCupidFrame[\"essay0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching and analyzing a single profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basis of natural language processing comes simply from analyzing a string. In this extent, it is natural to start out analysis by analyzing a single **document**, which is one observation in a language dataset. In this case, our **document** would be a single self-summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consideredSummary = selfSummaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a string, we can read it by a simple ```print``` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about me:<br />\n",
      "<br />\n",
      "i would love to think that i was some some kind of intellectual:\n",
      "either the dumbest smart guy, or the smartest dumb guy. can't say i\n",
      "can tell the difference. i love to talk about ideas and concepts. i\n",
      "forge odd metaphors instead of reciting cliches. like the\n",
      "simularities between a friend of mine's house and an underwater\n",
      "salt mine. my favorite word is salt by the way (weird choice i\n",
      "know). to me most things in life are better as metaphors. i seek to\n",
      "make myself a little better everyday, in some productively lazy\n",
      "way. got tired of tying my shoes. considered hiring a five year\n",
      "old, but would probably have to tie both of our shoes... decided to\n",
      "only wear leather shoes dress shoes.<br />\n",
      "<br />\n",
      "about you:<br />\n",
      "<br />\n",
      "you love to have really serious, really deep conversations about\n",
      "really silly stuff. you have to be willing to snap me out of a\n",
      "light hearted rant with a kiss. you don't have to be funny, but you\n",
      "have to be able to make me laugh. you should be able to bend spoons\n",
      "with your mind, and telepathically make me smile while i am still\n",
      "at work. you should love life, and be cool with just letting the\n",
      "wind blow. extra points for reading all this and guessing my\n",
      "favorite video game (no hints given yet). and lastly you have a\n",
      "good attention span.\n"
     ]
    }
   ],
   "source": [
    "print consideredSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Figure 1: A self-summary of an individual in our dataset._\n",
    "\n",
    "We can see a couple of things just from looking at this profile:\n",
    "\n",
    "* This man sounds extremely pretentious.\n",
    "\n",
    "* There are some misspellings due to the user-inputted aspects in this self-summary. most notably, the word \"simularities\" should probably be \"similarities.\"\n",
    "\n",
    "* Ther are several ```br``` tags within the document that do not add information to our understanding of the document. These tags are primarily for OkCupid to display the self-summary properly on their website.\n",
    "\n",
    "Thus, before we analyze this dataset, we need to do some data cleansing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and searching with Regular Expression (```regex```)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Regular Expression is defined as a sequence of characters that defines a search pattern. This search pattern is used to \"find\" and \"find and replace\" certain information in strings through string search algorithms. To give an example, say that I am interested in quantifying the narcissism found in the self-summary above. Perhaps I am interested in the number of times that \"i\" shows up in the summary. We represent this with the simple regular expression search query that accounts for the letter $i$ and then accounts for all potential punctuation that usually follows a lone $i$:\n",
    "\n",
    "```[ \\.,:;?!\\n]i[ \\.,:;?!\\n$]```\n",
    "\n",
    "This expression looks for some measure of punctuation, then an $i$ and then looks for a potential followup punctuation to indicate that is a lone $i$. Theses measures of punctuation can be a space, period, comma, colon, semi-colon, question mark, explanation point, or an end-of-line marker (```$```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ni ', ' i ', ' i\\n', ' i ', ' i\\n', ' i\\n', ' i ', ' i ']\n"
     ]
    }
   ],
   "source": [
    "import re #regular expression library in base Python\n",
    "#let us compile this for search\n",
    "iRe = re.compile(\"[ \\.,:;?!\\n]i[ \\.,:?!\\n]\")\n",
    "#then find all the times it occurs in the summary\n",
    "iObservanceList = iRe.findall(consideredSummary)\n",
    "print iObservanceList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numIs = len(iObservanceList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "numIs": "8"
    }
   },
   "source": [
    "We see that the speaker refers to himself in terms of \"i\" 8 times in this self-summary. This is actually more reasonable than most people when referring to themselves, but let's try to extend this regular expression to other self-centered terms. We will now search for\n",
    "\n",
    "```[ \\.,:;?!\\n](i|me)[ \\.,:?!\\n]```\n",
    "\n",
    "The ```|``` symbol represents an or operator for in a section. In this context, this regular expression is looking for either \"i\" or \"me\" followed by some punctuation in order to identify lone observations of ```i``` and ```me``` instead of appendages on other words (for example, ```i``` in ```intellectual``` and ```me``` in ```meandering```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['me', 'i', 'i', 'i', 'i', 'i', 'i', 'me', 'i', 'me', 'me', 'me', 'i']\n"
     ]
    }
   ],
   "source": [
    "selfCenteredRe = re.compile(\"[ \\.,:;?!\\n](i|me)[ \\.,:?!\\n]\")\n",
    "#find all observations of this regular expression\n",
    "selfObsList = selfCenteredRe.findall(consideredSummary)\n",
    "print selfObsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "numNarcissisticWords = len(selfObsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "numNarcissisticWords": "13"
    }
   },
   "source": [
    "We see that when we extend our search to include \"me\" as a possible pattern to recognize, we see that the number of self-referrals increases to 13. We can extend this to other aspects of the self-summary, and potentially more interesting patterns we want to find in the language.\n",
    "\n",
    "Regular Expressions can also be used to substitute particular components of the summary for data cleaning purposes. For instance, let us alter the mistake of \"simularities\" as \"similarities\" in the above summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about me:<br />\n",
      "<br />\n",
      "i would love to think that i was some some kind of intellectual:\n",
      "either the dumbest smart guy, or the smartest dumb guy. can't say i\n",
      "can tell the difference. i love to talk about ideas and concepts. i\n",
      "forge odd metaphors instead of reciting cliches. like the\n",
      "similarities between a friend of mine's house and an underwater\n",
      "salt mine. my favorite word is salt by the way (weird choice i\n",
      "know). to me most things in life are better as metaphors. i seek to\n",
      "make myself a little better everyday, in some productively lazy\n",
      "way. got tired of tying my shoes. considered hiring a five year\n",
      "old, but would probably have to tie both of our shoes... decided to\n",
      "only wear leather shoes dress shoes.<br />\n",
      "<br />\n",
      "about you:<br />\n",
      "<br />\n",
      "you love to have really serious, really deep conversations about\n",
      "really silly stuff. you have to be willing to snap me out of a\n",
      "light hearted rant with a kiss. you don't have to be funny, but you\n",
      "have to be able to make me laugh. you should be able to bend spoons\n",
      "with your mind, and telepathically make me smile while i am still\n",
      "at work. you should love life, and be cool with just letting the\n",
      "wind blow. extra points for reading all this and guessing my\n",
      "favorite video game (no hints given yet). and lastly you have a\n",
      "good attention span.\n"
     ]
    }
   ],
   "source": [
    "#make the re\n",
    "simRe = re.compile(\"simularities\")\n",
    "#then perform a sub\n",
    "filteredSummary = simRe.sub(\"similarities\",consideredSummary)\n",
    "print filteredSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Figure 2: The filtered summary after changing the stated spelling issue._\n",
    "\n",
    "As we can see, \"simularities\" was changed to \"similarities\" without us having to find the exact beginning and ending indices for the \"simularities\" mistake. We can continue this cleaning by altering an even larger interpretation issue: the ```br``` tags. These tags are primarily used for OkCupid to understand how to display the text, but they generally are not informative to the summary itself.\n",
    "\n",
    "We will remove these by building the regular expression\n",
    "\n",
    "```<.*>```\n",
    "\n",
    "The ```.``` is meant to represent any character available in the ASCII encoding framework. the ```*``` is meant to represent \"0 or more observations of the prior character or expression.\" In this case, this regular expression is asking to find strings that start with \"<\" and end with \">\" and feature any number of characters in between \"<\" and \">.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about me:\n",
      "\n",
      "i would love to think that i was some some kind of intellectual:\n",
      "either the dumbest smart guy, or the smartest dumb guy. can't say i\n",
      "can tell the difference. i love to talk about ideas and concepts. i\n",
      "forge odd metaphors instead of reciting cliches. like the\n",
      "similarities between a friend of mine's house and an underwater\n",
      "salt mine. my favorite word is salt by the way (weird choice i\n",
      "know). to me most things in life are better as metaphors. i seek to\n",
      "make myself a little better everyday, in some productively lazy\n",
      "way. got tired of tying my shoes. considered hiring a five year\n",
      "old, but would probably have to tie both of our shoes... decided to\n",
      "only wear leather shoes dress shoes.\n",
      "\n",
      "about you:\n",
      "\n",
      "you love to have really serious, really deep conversations about\n",
      "really silly stuff. you have to be willing to snap me out of a\n",
      "light hearted rant with a kiss. you don't have to be funny, but you\n",
      "have to be able to make me laugh. you should be able to bend spoons\n",
      "with your mind, and telepathically make me smile while i am still\n",
      "at work. you should love life, and be cool with just letting the\n",
      "wind blow. extra points for reading all this and guessing my\n",
      "favorite video game (no hints given yet). and lastly you have a\n",
      "good attention span.\n"
     ]
    }
   ],
   "source": [
    "tagRe = re.compile(\"<.*>\")\n",
    "filteredSummary = tagRe.sub(\"\",filteredSummary)\n",
    "print filteredSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Figure 3: Our filtered summary after all ```br``` tags have been removed._\n",
    "\n",
    "As we can see, we have cleaned the summary to a point where there are no tags whatsoever in the text. We can then use this edited summary within the main dataset. This process is essentially a form of data cleansing with text.\n",
    "\n",
    "If you would like to learn more about ```regex```, see the links in the [reference materials](#refMaterials)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Analysis on Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we are interested in the terms and expressions used by a person in a document. The atom of a document to some degree is a word, and so it seems appropriate to start analyzing the types of words used in this document.\n",
    "\n",
    "We will first do a \"tokenization\" of this document. Tokenization is when you simplify a document to just the sequence of words that make up the document. This requires us to split our document into a list based on certain punctuation marks (such as a ```.```, a new-line character, or a ```,```) and spaces, and then filtering our list into non-degenerate words (i.e. not the null string \"\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'me', '', '', 'i', 'would', 'love', 'to', 'think', 'that', 'i', 'was', 'some', 'some', 'kind', 'of', 'intellectual', '', 'either', 'the', 'dumbest', 'smart', 'guy', '', 'or', 'the', 'smartest', 'dumb', 'guy', '', \"can't\", 'say', 'i', 'can', 'tell', 'the', 'difference', '', 'i', 'love', 'to', 'talk', 'about', 'ideas', 'and', 'concepts', '', 'i', 'forge', 'odd', 'metaphors', 'instead', 'of', 'reciting', 'cliches', '', 'like', 'the', 'similarities', 'between', 'a', 'friend', 'of', \"mine's\", 'house', 'and', 'an', 'underwater', 'salt', 'mine', '', 'my', 'favorite', 'word', 'is', 'salt', 'by', 'the', 'way', '', 'weird', 'choice', 'i', 'know', '', '', 'to', 'me', 'most', 'things', 'in', 'life', 'are', 'better', 'as', 'metaphors', '', 'i', 'seek', 'to', 'make', 'myself', 'a', 'little', 'better', 'everyday', '', 'in', 'some', 'productively', 'lazy', 'way', '', 'got', 'tired', 'of', 'tying', 'my', 'shoes', '', 'considered', 'hiring', 'a', 'five', 'year', 'old', '', 'but', 'would', 'probably', 'have', 'to', 'tie', 'both', 'of', 'our', 'shoes', '', '', '', 'decided', 'to', 'only', 'wear', 'leather', 'shoes', 'dress', 'shoes', '', '', 'about', 'you', '', '', 'you', 'love', 'to', 'have', 'really', 'serious', '', 'really', 'deep', 'conversations', 'about', 'really', 'silly', 'stuff', '', 'you', 'have', 'to', 'be', 'willing', 'to', 'snap', 'me', 'out', 'of', 'a', 'light', 'hearted', 'rant', 'with', 'a', 'kiss', '', 'you', \"don't\", 'have', 'to', 'be', 'funny', '', 'but', 'you', 'have', 'to', 'be', 'able', 'to', 'make', 'me', 'laugh', '', 'you', 'should', 'be', 'able', 'to', 'bend', 'spoons', 'with', 'your', 'mind', '', 'and', 'telepathically', 'make', 'me', 'smile', 'while', 'i', 'am', 'still', 'at', 'work', '', 'you', 'should', 'love', 'life', '', 'and', 'be', 'cool', 'with', 'just', 'letting', 'the', 'wind', 'blow', '', 'extra', 'points', 'for', 'reading', 'all', 'this', 'and', 'guessing', 'my', 'favorite', 'video', 'game', '', 'no', 'hints', 'given', 'yet', '', '', 'and', 'lastly', 'you', 'have', 'a', 'good', 'attention', 'span', '']\n"
     ]
    }
   ],
   "source": [
    "#make our split\n",
    "sumWordList = re.split(\" |\\(|\\)|\\.|\\n|,|:|;\",filteredSummary)\n",
    "print sumWordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'me', 'i', 'would', 'love', 'to', 'think', 'that', 'i', 'was', 'some', 'some', 'kind', 'of', 'intellectual', 'either', 'the', 'dumbest', 'smart', 'guy', 'or', 'the', 'smartest', 'dumb', 'guy', \"can't\", 'say', 'i', 'can', 'tell', 'the', 'difference', 'i', 'love', 'to', 'talk', 'about', 'ideas', 'and', 'concepts', 'i', 'forge', 'odd', 'metaphors', 'instead', 'of', 'reciting', 'cliches', 'like', 'the', 'similarities', 'between', 'a', 'friend', 'of', \"mine's\", 'house', 'and', 'an', 'underwater', 'salt', 'mine', 'my', 'favorite', 'word', 'is', 'salt', 'by', 'the', 'way', 'weird', 'choice', 'i', 'know', 'to', 'me', 'most', 'things', 'in', 'life', 'are', 'better', 'as', 'metaphors', 'i', 'seek', 'to', 'make', 'myself', 'a', 'little', 'better', 'everyday', 'in', 'some', 'productively', 'lazy', 'way', 'got', 'tired', 'of', 'tying', 'my', 'shoes', 'considered', 'hiring', 'a', 'five', 'year', 'old', 'but', 'would', 'probably', 'have', 'to', 'tie', 'both', 'of', 'our', 'shoes', 'decided', 'to', 'only', 'wear', 'leather', 'shoes', 'dress', 'shoes', 'about', 'you', 'you', 'love', 'to', 'have', 'really', 'serious', 'really', 'deep', 'conversations', 'about', 'really', 'silly', 'stuff', 'you', 'have', 'to', 'be', 'willing', 'to', 'snap', 'me', 'out', 'of', 'a', 'light', 'hearted', 'rant', 'with', 'a', 'kiss', 'you', \"don't\", 'have', 'to', 'be', 'funny', 'but', 'you', 'have', 'to', 'be', 'able', 'to', 'make', 'me', 'laugh', 'you', 'should', 'be', 'able', 'to', 'bend', 'spoons', 'with', 'your', 'mind', 'and', 'telepathically', 'make', 'me', 'smile', 'while', 'i', 'am', 'still', 'at', 'work', 'you', 'should', 'love', 'life', 'and', 'be', 'cool', 'with', 'just', 'letting', 'the', 'wind', 'blow', 'extra', 'points', 'for', 'reading', 'all', 'this', 'and', 'guessing', 'my', 'favorite', 'video', 'game', 'no', 'hints', 'given', 'yet', 'and', 'lastly', 'you', 'have', 'a', 'good', 'attention', 'span']\n"
     ]
    }
   ],
   "source": [
    "#filter to have non-degenerate words\n",
    "filSumWordList = []\n",
    "for word in sumWordList:\n",
    "    if (len(word) > 0): filSumWordList.append(word)\n",
    "print filSumWordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the document is 234\n",
      "The number of unique words is 144\n"
     ]
    }
   ],
   "source": [
    "#this gave us number of words, let us get number of unique words\n",
    "numWords = len(filSumWordList)\n",
    "numUniqueWords = len(set(filSumWordList))\n",
    "print \"The length of the document is\", numWords\n",
    "print \"The number of unique words is\", numUniqueWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "len(filSumWordList)": "234",
     "numUniqueWords": "144"
    }
   },
   "source": [
    "We see that 234 non-unique words occur in this document. These are referred to as **tokens** in NLP. Of those non-unique words, we see that 144 unique words occur in this document. These distinct words are referred to as **types** in NLP. The relationship between tokens and types form the shallow basis of complexity in sentences.\n",
    "\n",
    "In this context, since the ratio of types to tokens is around .61, there is a surprising amount of diversity in vocabulary used in this document. Let us see what are the most common words used in this document. We will key words by count in the dictionary and then pick out the words found in the highest keys to get our most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating a map from i : list of words with frequency i\n",
    "wordCountDict = {}\n",
    "for word in set(filSumWordList): #look through unique words\n",
    "    numOccurences = 0\n",
    "    for token in filSumWordList:\n",
    "        if (word == token): #that means we have seen an occurences\n",
    "            numOccurences += 1\n",
    "    #after running through our word list, let us add it to the dictionary keyed\n",
    "    #by count\n",
    "    if (numOccurences in wordCountDict): #already have the list started\n",
    "        wordCountDict[numOccurences].append(word)\n",
    "    else: #start the list\n",
    "        wordCountDict[numOccurences] = [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : ['all', 'just', \"don't\", 'telepathically', 'hiring', 'still', 'yet', 'letting', 'underwater', 'similarities', 'only', 'willing', 'snap', 'smile', 'smart', 'good', 'conversations', 'kind', 'choice', 'game', 'five', 'know', 'kiss', 'instead', 'difference', 'cool', 'like', 'leather', 'stuff', 'either', 'guessing', 'old', 'tired', 'house', 'decided', 'dumb', 'are', 'year', 'our', 'odd', 'for', 'spoons', 'between', 'got', 'reading', 'lazy', 'attention', 'given', 'smartest', 'wear', 'dress', 'by', 'both', 'weird', 'think', 'extra', 'intellectual', 'rant', 'laugh', 'dumbest', 'lastly', 'your', 'little', 'span', 'blow', 'was', 'tell', 'friend', 'that', 'am', 'tying', 'probably', 'hints', 'myself', 'word', 'this', 'work', 'as', 'considered', 'while', 'silly', 'can', 'concepts', 'video', 'at', \"mine's\", 'is', 'mind', 'mine', 'deep', 'an', 'say', 'bend', 'hearted', 'seek', 'everyday', 'funny', 'no', 'cliches', 'things', 'ideas', 'productively', 'forge', 'tie', 'out', 'reciting', 'most', 'wind', 'light', 'or', 'points', \"can't\", 'serious', 'talk']\n",
      "2 : ['should', 'better', 'guy', 'favorite', 'metaphors', 'would', 'way', 'life', 'but', 'in', 'able', 'salt']\n",
      "3 : ['some', 'really', 'with', 'my', 'make']\n",
      "4 : ['shoes', 'about', 'love']\n",
      "5 : ['be', 'me']\n",
      "6 : ['of', 'and', 'have', 'a', 'the']\n",
      "8 : ['you', 'i']\n",
      "13 : ['to']\n"
     ]
    }
   ],
   "source": [
    "#print this out by key\n",
    "for count in wordCountDict:\n",
    "    print count, \":\", wordCountDict[count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_Figure 4: Our words keyed by their frequency in the document._\n",
    "\n",
    "We see that personal references to \"you\" and \"i\" seem to show up frequenty, and words such as \"love\" and \"me\" also have a sizable number of occurences. However, we do notice that there are a lot of words that show up generally infrequently, and only a small number of words that repeat themselves at the very least. We will see this behavior in our word distribution occur in our main corpus in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summaryStatistics\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics on a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then extend our analysis from the single document level to a multi-document level. In NLP, we refer to a collection of documents as a **corpus**. If you study this practice further, you will see that a collection of collections of documents is referred to as **corpora**.\n",
    "\n",
    "We will start our macro-analysis by simply counting the words that occur in all self-summaries and studying the commonality of certain words in these OkCupid profiles.\n",
    "\n",
    "We will import several packages that may be unfamiliar. ```nltk``` is the [Natural Language Toolkit](http://www.nltk.org), which is perhaps the most important package in modern programming for language analysis and document processing. ```collections``` allows us to use data structures that not only provides us with a dictionary-like object, but also keeps track of some measure of order in our dictionary; in our case, it allows us to keep track of the most frequent words in our corpus. ``StringIO`` gives us a flexible mechanism to write the entire corpus (all self-summaries) out into one readable string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imports discussed\n",
    "import nltk #for relevant corpora\n",
    "import collections as co #for ordered dictionary\n",
    "import StringIO #for string manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about me:<br />\n",
      "<br />\n",
      "i would love to think that i was some some kind of intellectual:\n",
      "either the dumbest smart guy, or the smartest dumb guy. can't say i\n",
      "can tell the difference. i love to talk about ideas and concepts. i\n",
      "forge odd metaphors instead of reciting cliches. like the\n",
      "simularities between a friend of mine's house and an underwater\n",
      "salt mine. my favorite word is salt by the way (weird choice i\n",
      "know). to me most things in life are better as metaphors. i seek to\n",
      "make myself a little better everyday, in some productively lazy\n",
      "way. got tired of tying my shoes. considered hiring a five year\n",
      "old, but would probably have to tie both of our shoes... decided to\n",
      "only wear leather shoes dress shoes.<br />\n",
      "<br />\n",
      "about you:<br />\n",
      "<br />\n",
      "you love to have really serious, really deep conversations about\n",
      "really silly stuff. you have to be willing to snap me out of a\n",
      "light hearted rant with a kiss. you don't have to be funny, but you\n",
      "have to be able to make me laugh. you should be able to bend spoons\n",
      "with your mind, and telepathically make me smile while i am still\n",
      "at work. you should love life, and be cool with just letting the\n",
      "wind blow. extra points for reading all this and guessing my\n",
      "favorite video game (no hints given yet). and lastly you have a\n",
      "good attention span.\n",
      "i am a chef: this is what that means.<br />\n",
      "1. i am a workaholic.<br />\n",
      "2. i love to cook regardless of whether i am at work.<br />\n",
      "3. i love to drink and eat foods that are probably really bad for\n",
      "me.<br />\n",
      "4. i love being around people that resemble line 1-3.<br />\n",
      "i love the outdoors and i am an avid skier. if its snowing i will\n",
      "be in tahoe at the very least. i am a very confident and friendly.\n",
      "i'm not interested in acting or being a typical guy. i have no time\n",
      "or patience for rediculous acts of territorial pissing. overall i\n",
      "am a very likable easygoing individual. i am very adventurous and\n",
      "always looking forward to doing new things and hopefully sharing it\n",
      "with the right person.\n",
      "i'm not ashamed of m\n"
     ]
    }
   ],
   "source": [
    "#create a writeable string object\n",
    "stringWriteTerm = StringIO.StringIO()\n",
    "#write all summaries to the string write term\n",
    "for essay in filteredOkCupidFrame[\"essay0\"]:\n",
    "    stringWriteTerm.write(essay)\n",
    "    #add some space for the next essay\n",
    "    stringWriteTerm.write(\"\\n\")\n",
    "#get the full string from the writer\n",
    "summaryString = stringWriteTerm.getvalue()\n",
    "stringWriteTerm.close()\n",
    "#lower the lettering\n",
    "summaryString = summaryString.lower()\n",
    "numCharsToView = 2000\n",
    "print summaryString[0:numCharsToView]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#then spit into a word list\n",
    "summaryWordList = re.split(\"\\.| |,|;|:|-|\\n|<.*>\",summaryString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a string of the entire ordered corpus, we will now filter to only include legal words in the English dictionary and remove any potential stopwords. Stopwords are words that do not indicate significance in understanding the contents of a sentence. These are typically words like \"the\", \"to\", \"on.\" For an understanding of how language practitioners choose these stopwords, please see the [reference materials](#refMaterials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://www.nltk.org/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for downloading datasets\n",
    "nltk.download()\n",
    "#d\n",
    "#words\n",
    "#d\n",
    "#stopwords\n",
    "#q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get legal words and get stop words\n",
    "legalWordSet = set(nltk.corpus.words.words())\n",
    "stopWordSet = set(nltk.corpus.stopwords.words())\n",
    "#then make our filtration\n",
    "filteredSumWordList = [word for word in summaryWordList\n",
    "                           if word in legalWordSet and\n",
    "                              word not in stopWordSet]\n",
    "numWordsToView = numCharsToView\n",
    "#print filteredSumWordList[0:numWordsToView]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#then count the frequency of words in a collection\n",
    "filteredWordCounter = co.Counter(filteredSumWordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```filteredWordCounter``` object is what we would call an ordered dictionary. A dictionary in ```python``` is a set of key-value pairs where given a particular key as an index, the dictionary will return its respective value. The counter object is a slight refinement of a dictionary: it defines an ordering over the keys based on their integer values. In our case, given the list of strings it sees, it orders said strings by their frequency in the list. This allows us to both capture the number of words in the list (namely, the sum of the values) and the number of distinct words in the list (namely, the number of keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-distinct words is 2479674\n",
      "Number of distinct words is  23107\n"
     ]
    }
   ],
   "source": [
    "numNonDistinctWords = sum(filteredWordCounter.values())\n",
    "numDistinctWords = len(filteredWordCounter.keys())\n",
    "print \"Number of non-distinct words is\", numNonDistinctWords\n",
    "print \"Number of distinct words is \", numDistinctWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "numDistinctWords": "22940",
     "numNonDistinctWords": "2449319"
    }
   },
   "source": [
    "We see that we have 2479674 tokens that occur in our document corpus.We also have 23107 types that occur in our corpus. It is important to note that the number of types is much smaller than the number of tokens in this context, which suggests that our vocabulary is not extremely rich in this context.\n",
    "\n",
    "Let us study the full word distribution. We will place the relative frequency of words (i.e. the density of the distribution), on the rank of the words, where the rank of a word is $i$ if that word is the $i$th most frequent word in the corpus.\n",
    "\n",
    "We will use the ```pyplot``` package in the ```matplotlib``` library for visualizing this distribution. We use a decorator feature to ensure that our plots stay within the Jupyter environment. We will also use the ```numpy``` package to gain some mathematical functions, in particular the ```log``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #for plotting\n",
    "#for inline plotting with Jupyter notebook\n",
    "%matplotlib inline\n",
    "import numpy as np #for some mathematical features\n",
    "#make series of word frequency ordered by most common words\n",
    "#print filteredWordCounter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Word  Frequency       Density\n",
      "0                 love      46967  1.894080e-02\n",
      "1                 like      43942  1.772088e-02\n",
      "2                 life      27290  1.100548e-02\n",
      "3               people      25806  1.040701e-02\n",
      "4                  new      22369  9.020944e-03\n",
      "5                 time      22116  8.918914e-03\n",
      "6                 good      21116  8.515636e-03\n",
      "7                enjoy      17146  6.914619e-03\n",
      "8                  get      15498  6.250015e-03\n",
      "9              looking      14487  5.842300e-03\n",
      "10                know      14440  5.823346e-03\n",
      "11              really      14139  5.701959e-03\n",
      "12             someone      13929  5.617271e-03\n",
      "13                 one      13287  5.358366e-03\n",
      "14                 fun      13268  5.350703e-03\n",
      "15               going      12904  5.203910e-03\n",
      "16                work      12611  5.085749e-03\n",
      "17               would      12086  4.874028e-03\n",
      "18                much      11307  4.559874e-03\n",
      "19                  go      11299  4.556647e-03\n",
      "20              person      10971  4.424372e-03\n",
      "21               think      10744  4.332828e-03\n",
      "22                make      10696  4.313470e-03\n",
      "23               music      10635  4.288870e-03\n",
      "24              always      10511  4.238864e-03\n",
      "25                well       9861  3.976732e-03\n",
      "26                back       9458  3.814211e-03\n",
      "27                find       9395  3.788804e-03\n",
      "28              pretty       9327  3.761382e-03\n",
      "29                live       9020  3.637575e-03\n",
      "...                ...        ...           ...\n",
      "23077             vier          1  4.032788e-07\n",
      "23078      econometric          1  4.032788e-07\n",
      "23079            truer          1  4.032788e-07\n",
      "23080  heterosexuality          1  4.032788e-07\n",
      "23081           tousle          1  4.032788e-07\n",
      "23082        piteously          1  4.032788e-07\n",
      "23083          gassing          1  4.032788e-07\n",
      "23084          octagon          1  4.032788e-07\n",
      "23085      equidistant          1  4.032788e-07\n",
      "23086             wren          1  4.032788e-07\n",
      "23087         unsigned          1  4.032788e-07\n",
      "23088           cutler          1  4.032788e-07\n",
      "23089         vortices          1  4.032788e-07\n",
      "23090       unbuttered          1  4.032788e-07\n",
      "23091             lear          1  4.032788e-07\n",
      "23092       friskiness          1  4.032788e-07\n",
      "23093            mania          1  4.032788e-07\n",
      "23094     thoroughfare          1  4.032788e-07\n",
      "23095         warranty          1  4.032788e-07\n",
      "23096            bugle          1  4.032788e-07\n",
      "23097            maize          1  4.032788e-07\n",
      "23098         footwork          1  4.032788e-07\n",
      "23099            wight          1  4.032788e-07\n",
      "23100         windfall          1  4.032788e-07\n",
      "23101           spanky          1  4.032788e-07\n",
      "23102             boor          1  4.032788e-07\n",
      "23103        hippocras          1  4.032788e-07\n",
      "23104      grandiosity          1  4.032788e-07\n",
      "23105        northerly          1  4.032788e-07\n",
      "23106       untalented          1  4.032788e-07\n",
      "\n",
      "[23107 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#make series of word frequency ordered by most common words\n",
    "wordFrequencyFrame = pd.DataFrame(filteredWordCounter.most_common(),\n",
    "                                  columns = [\"Word\",\"Frequency\"])\n",
    "wordFrequencyFrame[\"Density\"] = (wordFrequencyFrame[\"Frequency\"] /\n",
    "                                    sum(wordFrequencyFrame[\"Frequency\"]))\n",
    "print wordFrequencyFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1215f9a50>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGcCAYAAAD6VfsBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZLUBv4fCLWilBWv5X0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121df9550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#then plot rank-density plot\n",
    "#for the sake of easier visuals, we will log the rank\n",
    "desiredLineWidth = 3\n",
    "plt.plot(np.log(wordFrequencyFrame.index+1),wordFrequencyFrame[\"Density\"],\n",
    "         lw = desiredLineWidth)\n",
    "plt.xlabel(\"Log-Rank\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Log(Rank)-Density Plot\\nFor Words in our Summary Corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Figure 5: Distribution of our words on the $\\log(Rank)$ of the words._\n",
    "\n",
    "We see that while we have around $e^{10} \\approx 22000$ words in our distribution, the density peters out after the $e^6 \\approx 400$ most frequent words. This suggests that we have many words that occur extremely rarely in our dataset and only a few words that occur relatively often. This is referred to as a distribution that behaves as a **Zipfian distribution**, which is a type of distribution where you have a few events that occur very often and many events that occur rarely. This distribution is fundamental to understanding the language-generating process within NLP.\n",
    "\n",
    "Let us see what are top $10$ most frequent words are. We will import some visualization features to embed a table within our Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>love</td>\n",
       "      <td>46967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>like</td>\n",
       "      <td>43942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>life</td>\n",
       "      <td>27290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>people</td>\n",
       "      <td>25806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>new</td>\n",
       "      <td>22369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>time</td>\n",
       "      <td>22116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>good</td>\n",
       "      <td>21116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>enjoy</td>\n",
       "      <td>17146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>get</td>\n",
       "      <td>15498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>looking</td>\n",
       "      <td>14487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#grab top ten words\n",
    "topLev = 10\n",
    "topTenWordFrame = wordFrequencyFrame.iloc[0:topLev,:].loc[:,\n",
    "                                                        [\"Word\",\"Frequency\"]]\n",
    "#then display\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(topTenWordFrame.to_html(index = False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Table 1: Our top ten most frequent words by their count frequency._\n",
    "\n",
    "We see that words that display some level affection are emphasized, such as \"like\" and \"love.\" Words with positive connotations also seem to occur often, as shown by the frequency of \"good\" and \"enjoy.\" There is also a sense of discovery of new individuals that pervades our vocabulary, as shown by the occurence words such as \"looking\", \"new\", and \"people.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"languageModels\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin trying to predict the age of a writer, we must first introduce the way we use language in the prediction process. **Features** are the components of your data you use to predict your **outcome** variable. In this context, our features are the language, and our outcome is age. However, how do I go about encoding language as a feature? This is the fundamental question of **feature engineering**.\n",
    "\n",
    "In NLP, feature engineering often boils down to a way of representing each document in a corpus as some numerical vector. This numerical vector is referred to as the **encoding** of a document. Some would argue that encoding is purely for practical purposes, and would argue that an encoding of a document is arbitrary. However, NLP practitioners have shown how encodings place important assumptions on how a document was generated. The way we represent a document-generating process is referred to as **language modeling**.\n",
    "\n",
    "Language modeling could be a lecture unto itself (which may be a good potential followup). However, for the time being, we will describe one of the most simple models of language generation: the **unigram**.\n",
    "\n",
    "let us describe a document of lenght $n$ as a sequence of words\n",
    "\n",
    "$$W = (w_1,w_2,...,w_n).$$\n",
    "\n",
    "A simple way to describe how this document is generated is through some form of distribution over the words that exist in the document. The unigram model assumes that every word in the sequence is generated **independently of the words before it and the words after it**. For example, consider the second word that occurs in the sequence $w_2$. What we are stating is that the probability of seeing $w_2$ occur in the document is independent of whatever other words I see in this document. Mathematically,\n",
    "\n",
    "$$P(w_2 | w_1,w_3,...,w_n) = P(w_2)$$\n",
    "\n",
    "This essentially leads to the implication that the probability of seeing a document is equal to the product of the likelihood of each word occuring in this document:\n",
    "\n",
    "$$P(W) = P(w_1,w_2,...,w_n) = P(w_1)P(w_2)...P(w_n) = \\prod_{i = 1}^n P(w_i).$$\n",
    "\n",
    "In this extent, the sufficient information we need to know to explain how a document is generated is just a **probability distribution over all words in our vocabulary**.\n",
    "\n",
    "Let us simulate how this model generates language. We will take our current word distribution (see [Summary Statistics](#summaryStatistics)) and simulate a process of pulling our language from a hat containing that distribution. This is why this language model is referred to commonly as a **bag of words**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build bag of words distribution\n",
    "bow = []\n",
    "for word in filteredWordCounter:\n",
    "    #make observation list by multiplying by count\n",
    "    wordObsList = [word] * filteredWordCounter[word]\n",
    "    #then extend bow as such\n",
    "    bow.extend(wordObsList)\n",
    "#print bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build sampling function\n",
    "import random #for sampling\n",
    "def generateLanguage(numWords,bow):\n",
    "    #helper that generates our language using out bag of words\n",
    "    newPhraseList = []\n",
    "    for i in xrange(numWords):\n",
    "        #sample a word from our distribution\n",
    "        newWordList = random.sample(bow,1)\n",
    "        newPhraseList.extend(newWordList)\n",
    "    newPhrase = \" \".join(newPhraseList)\n",
    "    return newPhrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eastern assets\n",
      "easy find see partner\n",
      "lot fancy little much funny engage important baking\n",
      "towards together humor instead new night kindness every wisely short towards anyone totally pro something working\n"
     ]
    }
   ],
   "source": [
    "#some examples\n",
    "sentenceLenList = [2,4,8,16]\n",
    "for sentenceLen in sentenceLenList:\n",
    "    print generateLanguage(sentenceLen,bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our language model, these are dating profiles that can be generated. However, do these represent realistic forms of communication? Do these look like realistic self-summaries for dating profiles? It is apparent that the answer is no. This is an extremely unrealistic model for how our langauge is generated, most notably because each word is generated independently of each other in the document. Given the many phrasal dependencies that occur in real documents, this is not a realistic assumption to make. That being said, we will see how using this model of language actually performs reasonably well for the task of prediction.\n",
    "\n",
    "Given that we use a unigram model of language, the only information I need to know to inform a documents likelihood of generation, i.e. $P(W),$ is simply the frequencies of the words that appear in the document. Thus, This leads to a bag of words encoding that maps each document $W$ to an encoding $D_{W}.$ The encoding $D_{W}$ is a vector of length $|V|$, where $V$ is the vocabulary of our corpus. If we define an ordering of words over our vocabulary, we can say that the $i$th component of $D_{W}$ is\n",
    "\n",
    "$$D_{W,i} = \\text{number of times word }i\\text{ appears in document }W.$$\n",
    "\n",
    "This defines the way we will encode the documents in our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the essential reasons why NLP is significant in data science is that language has often been shown to be an effective predictor of structural and environmental components in the world. In our case, we might be interested in seeing how language of a writer can help us predict the age of the writer. Let us first take a look at the distribution of ages available in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1218d7490>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAGHCAYAAABBFAMBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1215f93d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#mkae a histogram\n",
    "newPlot = plt.hist(filteredOkCupidFrame[\"age\"])\n",
    "plt.xlabel(\"Age of User\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of User Ages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Figure 6: Distribution of the ages of users in this dataset._\n",
    "\n",
    "We see that there is a large concentration of users in the $[20,30]$ age range and a slight tail of users extending into older ages. This is expected due to the fact that OkCupid specifically targets young adults for its main user base.\n",
    "\n",
    "We are interested in seeing if we can use the language content in the summary of a user to predict some measurement of age. For the sake of simplification, let us try to predict whether someone is within the millenial age range. For a beginner's prediction problem, it is much easier to deal with predicting a binary outcome than a continous scale of outcomes. For the sake of further simplification, we will assume that a millenial is anyone under of $28.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate the variable from our stated cutoff\n",
    "filteredOkCupidFrame[\"isMillenial\"] = 0\n",
    "millenialCutoff = 28\n",
    "filteredOkCupidFrame.loc[filteredOkCupidFrame[\"age\"] < millenialCutoff,\n",
    "                         \"isMillenial\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical **supervised learning** task is initialized as such:\n",
    "\n",
    "Given a target variable $Y$ and a feature set $X$, We are looking for a function $f$ such that\n",
    "\n",
    "$$f : X \\rightarrow Y.$$\n",
    "\n",
    "In the task of **regression**, $Y$ is some continuous set of variables (typically $\\mathbb{R}$). In the task of **classification**, $Y$ is some discrete set of variables. In our case, $Y = \\{0,1\\},$ where $1$ represents the instance where someone is a millenial and $0$ represents the instance where someone is not a millenial. This would be considered a **binary classification** task.\n",
    "\n",
    "For this task, we will introduce a model (or, to some degree, a family of functions) referred to as **logistic regression**. Say that we are interested in estimating the probability that one is a millenial given our feature set, i.e.\n",
    "\n",
    "$$P(isMillenial|X).$$\n",
    "\n",
    "Logistic regression assumes that this function takes on a **sigmoidal** form:\n",
    "\n",
    "$$P(isMillenial|X) = \\frac{1}{1 + e^{-r(X)}},$$\n",
    "\n",
    "where $r : X \\rightarrow \\mathbb{R}$ is some regression function. This function $r$ is the main reason why logistic regression is referred to as regression despite being used for classification purposes.\n",
    "\n",
    "Our model takes an additional assumption: that $r$ is a function that is linear in our feature set. Take $X_i$ to be the frequency of word $i$ in our document, $i \\in V.$ We see that\n",
    "\n",
    "$$r(X) = \\sum_{i = 1}^{|V|} \\beta_i X_i,$$\n",
    "\n",
    "where $\\beta_i$ is some weight, $i \\in V.$ The main question that we want to know is, what are the optimal $\\beta_i$'s to fit this model?\n",
    "\n",
    "We won't have enough time to discuss the training algorithm used to find our weights for $r$, but for the time being, we will say that we will train this logistic regression with an objective function featuring an $L_1$ penalty. An **objective function** is some function that we either minimize or maximize in a way to find our weights. In that sense, the objective function is some measurement as to **how well our model is performing relative to other models**. We would prefer this objective function to be one that measures **how well our model is fitting the data**. The $L_1$ penalty is a component of the objective function that penalizes models that place to large of weights. To some degree, this allows our objective function to prefer models that fit the data well, but penalize models that are overly complex and assign too many weights to too many features. The $L_1$ penalty is useful **when our feature set is very large.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Scikit-learn ```sklearn``` to build the bag-of-words representation of our summaries discussed in the [previous section](#languageModels). We will then use a Logistic Regression with $L_1$ penalty for our predictive pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import our count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#make a vocab dictionary\n",
    "counterList = filteredWordCounter.most_common()\n",
    "vocabDict = {}\n",
    "for i in xrange(len(counterList)):\n",
    "    rankWord = counterList[i][0]\n",
    "    vocabDict[rankWord] = i\n",
    "#initialize vectorizer\n",
    "vectorizer = CountVectorizer(min_df=1,stop_words=stopWordSet,\n",
    "                             vocabulary = vocabDict)\n",
    "#then fit and transform our summaries\n",
    "bagOfWordsMatrix = vectorizer.fit_transform(filteredOkCupidFrame[\"essay0\"])\n",
    "#print bagOfWordsMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get language frame\n",
    "langFrame = pd.DataFrame(bagOfWordsMatrix.toarray(),\n",
    "                         columns = vectorizer.get_feature_names())\n",
    "#display(langFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import linear model\n",
    "import sklearn.linear_model as lm\n",
    "#build model\n",
    "initialLinearMod = lm.LogisticRegression(penalty = \"l1\")\n",
    "initialLinearMod.fit(langFrame,filteredOkCupidFrame[\"isMillenial\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fit our model, let us see how well we are currently fitting our dataset. We will first test for accuracy of our model on the decision rule that if our predicted probability is above $.5$, we will predict that an individual is a millenial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#make predictions\n",
    "predictionVec = initialLinearMod.predict(langFrame)\n",
    "filteredOkCupidFrame[\"predictedLabel\"] = list(predictionVec)\n",
    "#print filteredOkCupidFrame[\"predictedLabel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.789867978921\n"
     ]
    }
   ],
   "source": [
    "#then test for accuracy\n",
    "accurateFrame = filteredOkCupidFrame[filteredOkCupidFrame[\"isMillenial\"] ==\n",
    "                                     filteredOkCupidFrame[\"predictedLabel\"]]\n",
    "accuracy = float(accurateFrame.shape[0]) / filteredOkCupidFrame.shape[0]\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our model is predicting accurately around $78.99\\% \\approx 80\\%$ of the time on our current dataset. This means that on the dataset it is using to **train**, it makes a predictive mistake on average about $1$ for every $5$ predictions. Depending on the context, this might not be an ideal fit for the data. That being said, given that this is an accuracy rate built on a relatively nave feature set (see [Language Modeling](#languageModels)), we are performing surprisingly well despite rather simple methods that we are using.\n",
    "\n",
    "Let us now look at the **confusion matrix** of our predictions. A confusion matrix is a matrix that compares our predicted outcomes on our actual labels. In particular, row $i$ indicates instances where our model predicts label $i$, and column $j$ indicates instances where our model predicts label $j$. When put together, Cell $i,j$ of the confusion matrix contains the number of observations where we predict label $i$ on outcomes that are actually labeled $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30821</td>\n",
       "      <td>7531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3913</td>\n",
       "      <td>12196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1\n",
       "0  30821   7531\n",
       "1   3913  12196"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#confusion matrix\n",
    "confusionMat = pd.DataFrame({0:[0,0],1:[0,0]})\n",
    "#get our indices\n",
    "rowIndices = list(confusionMat.index)\n",
    "colIndices = list(confusionMat.columns)\n",
    "#then run through theses\n",
    "for row in rowIndices:\n",
    "    for col in colIndices:\n",
    "        #grab observations with row predictedLabel and col actual label\n",
    "        givenObs = filteredOkCupidFrame[\n",
    "                (filteredOkCupidFrame[\"predictedLabel\"] == row) &\n",
    "                (filteredOkCupidFrame[\"isMillenial\"] == col)]\n",
    "        #then just get the number of observations in this situation\n",
    "        numObs = givenObs.shape[0]\n",
    "        #then store is\n",
    "        confusionMat.loc[row,col] = numObs\n",
    "#then display our confusion matrix\n",
    "display(confusionMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have about $7526$ observations that we predicted as not millenials (label $0$), but were actually millenials (label $1$). This is referred to as **false negatives** in a binary classification problem. We also see that we have about $3912$ observations that we predicted as millenials (label $1$), but were actually non-millenials (label $0$). This is referred to as **false positives** in a binary classification problem. In this context, we see that our false negative rate is slightly larger than our false positive rate in magnitude, but it is very important to note that this is a large portion of the millenials we are predicting incorrectly upon (about $\\frac{7526}{7526 + 12201} \\cdot 100 \\approx 38\\%$). To some degree, it's important to note that we have way fewer millenials in this dataset by our labeling hypothesis than non-millenials. This leads to what we call an **imbalanced classes** problem in binary classification. We will discuss more about the potential impact of this issue in our [Next Questions](#nextQuestions) section.\n",
    "\n",
    "Let us take a look at the coefficients fit for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of betas that equal 0 are 16673\n"
     ]
    }
   ],
   "source": [
    "#look at coefficients\n",
    "coefVec = initialLinearMod.coef_[0] #because logistic regression syntax\n",
    "coefFrame = pd.DataFrame({\"Feature Name\": vectorizer.get_feature_names(),\n",
    "                         \"Coefficient\":coefVec})\n",
    "#get number of 0 coefficients\n",
    "zeroCoefFrame = coefFrame[coefFrame[\"Coefficient\"] == 0]\n",
    "numZeroCoeffs = zeroCoefFrame.shape[0]\n",
    "print \"The number of betas that equal 0 are\", numZeroCoeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compare this to the fact there are $\\approx 22000$ words in our corpus, we see that approximately $76\\%$ of the words we considered in this initial model have no predictive effect in our current fitted model. Thus, this is an extremely sparse model in terms of our coefficients, which shows the strength of the $L_1$ penalty. It is also important to tie this back to the original sparsity in our word distribution. Since the $400$ most frequent words take up most of our word distribution, we have many words that occur so rarely that they do not have any predictive effect.\n",
    "\n",
    "Let us look at our non-zero coefficients. For the time we have, we will interpret the coefficients with the largest magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#consider nonzero coefficients\n",
    "nonZeroCoeffFrame = coefFrame[coefFrame[\"Coefficient\"] != 0]\n",
    "#get absolute magnitude of coefficients\n",
    "nonZeroCoeffFrame[\"absCoeff\"] = np.abs(nonZeroCoeffFrame[\"Coefficient\"])\n",
    "#order by absolute value\n",
    "nonZeroCoeffFrame = nonZeroCoeffFrame.sort_values(\"absCoeff\",\n",
    "                                                  ascending = False)\n",
    "#then display values\n",
    "#display(HTML(nonZeroCoeffFrame.to_html(index = False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret the meaning of these coefficients, let us look back at the mathematical representation of the model we fit:\n",
    "\n",
    "$$P(isMillenial | X) = \\frac{1}{1 + e^{-\\sum_{i = 1}^{|V|} \\beta_i X_i}}.$$\n",
    "\n",
    "We see that if our coefficient for word $i$ ($\\beta_i$) is positive, this makes our denominator smaller and pushes our prediction closer to the $isMillenial$ direction. Similarly, if the coefficient is negative, this will make our denominator bigger and push our predictions away from the $isMillenial$ direction.\n",
    "\n",
    "We see that \"pregnancy\" seems to be a word that has the highest magnitude in its coefficient, which suggests that pregnancy seems to be an important indicator for whether the profile writer is a millenial. This is a bit peculiar, given that pregnancy seems to be something that is uninteresting to younger millenials who tend to not be interested in \"settling down\" any time soon. That being said, we see that \"unyielding\" and \"unsatisfied\" seem to also be strong indicators of whether a writer is a millenial or not. To some degree, this paints an interesting narrative: that millenials can often look at relationships as a new experience, something where they can have \"unyielding\" discover of themselves compared to a lack of satisfaciton in previous relationships. This would then beg the question as to whether these are also focuses for non-millenials.\n",
    "\n",
    "That being said, it is often useful to look at a set of example predictions to see how flexible our model might be in other dating contexts. Let's take a look at one profile we accurately predict is a millenial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The age of this profile is : 26\n",
      "Their Self-Summary:\n",
      "\n",
      "to sum myself in whole: i have adventurous tendencies with a\n",
      "somewhat vulgar flare (i hope you can take a joke-offensive or\n",
      "not).<br />\n",
      "<br />\n",
      "i'm a constant seeker of discovering something new, whether it be a\n",
      "different restaurant, culture, or music (anything really). i tend\n",
      "to have an eclectic personality.<br />\n",
      "<br />\n",
      "i volunteer and participate with a lot of activities with my\n",
      "community. i also tend to seek those who have a story, any story.\n",
      "i'm all ears! i want to learn from you, hear what you have to say,\n",
      "and relate it to my own personal experience.<br />\n",
      "<br />\n",
      "i'm a bit of a nerd and like to watch the history, military, and\n",
      "discovery channel. something about militaristic strategies intrigue\n",
      "me.<br />\n",
      "<br />\n",
      "i'm a bit of a tom boy, so i indulge in shooting guns, playing\n",
      "video games, and working out consistently. i also have the guilty\n",
      "pleasure of watching ridiculous comedy (family guy, curb your\n",
      "enthusiasm, it's always sunny ...just to name a few)\n",
      "\n",
      "Our Model Predicts this person is a millenial\n"
     ]
    }
   ],
   "source": [
    "millenialProfileInd = 17 #checked this earlier\n",
    "print \"The age of this profile is :\", filteredOkCupidFrame.loc[millenialProfileInd,\"age\"]\n",
    "print \"Their Self-Summary:\"\n",
    "print\n",
    "givenEssay = filteredOkCupidFrame.loc[millenialProfileInd,\"essay0\"]\n",
    "print givenEssay\n",
    "#make our predictions\n",
    "newBOW = vectorizer.transform([givenEssay])\n",
    "newPrediction = initialLinearMod.predict(newBOW)\n",
    "print\n",
    "if (newPrediction[0] == 1):\n",
    "    print \"Our Model Predicts this person is a millenial\"\n",
    "else:\n",
    "    print \"Our Model Predicts this person is not a millenial\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our model predicts this person is a millenial, and when we look at the contents of the summary, it is not extremely surprising. We see that this individual enjoys video games (a trope of millenials), and discusses a lot about their nerd-oriented hobbies.\n",
    "\n",
    "However, let us see how our model performs in a different context. Let's take a look at my old Tinder bio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "#try my Tinder profile\n",
    "michaelTinderSummary = \"\"\"Senior at Carnegie Mellon, starting work in Boston\n",
    "next summer as a data scientist at place X :D\n",
    "\n",
    "I enjoy books, singing, coffee, and polar bears :D\"\"\"\n",
    "#lower it\n",
    "michaelTinderSummary = michaelTinderSummary.lower()\n",
    "#then predict\n",
    "personalBOW = vectorizer.transform([michaelTinderSummary])\n",
    "personalPrediction = initialLinearMod.predict(personalBOW)\n",
    "print personalPrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, this summary makes an innaccurate prediction, since I am $21$ and yet it suggests that I am not a Millenial. That being said, given the many emoticons and proper nouns featured in my profile, it is likely that there are many words that were not picked up in the features since they were not in the **original vocabulary**.\n",
    "\n",
    "Let us take a look at another individual's Tinder profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "#try a friend's Tinder profile\n",
    "friendTinderSummary = \"\"\"Insert a friend's profile here\"\"\"\n",
    "#lower it\n",
    "friendTinderSummary = friendTinderSummary.lower()\n",
    "#then predict\n",
    "friendBOW = vectorizer.transform([friendTinderSummary])\n",
    "friendPrediction = initialLinearMod.predict(friendBOW)\n",
    "print friendPrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that even for my millenial friend, we are still predicting that they are not a millenial! There are a couple components to this that we need to consider:\n",
    "\n",
    "* Our model is rather inflexible to new words. We still do not have a way of encoding words that are out of our vocabulary, and that can severely affect predictions like on my Tinder profile.\n",
    "\n",
    "* Our model is rather inflexible to context. The nature of what individuals write on their profiles on [OkCupid](https://www.okcupid.com) can be very different than what individuals write on a [Tinder](https://www.gotinder.com) profile. To this extent, it might be important for us to consider how our model might be effective on out-of-corpus situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nextQuestions\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recap what we have done in this workshop.\n",
    "\n",
    "* We figured out a way of ingesting and cleansing using the ```pandas``` package and the regular expression framework of pattern recognition within text.\n",
    "\n",
    "* We used a set of language processing tools to develop summary statistics on the words found at the document level and the corpus level.\n",
    "\n",
    "* We then walked through a method of how we can encode our language to be a set of numerical features in a **prediction pipeline**, a structure that takes in a set of inputs (or features) and gives us a set of predictive outputs.\n",
    "\n",
    "* We then used this encoding to predict the age of the writer of said documents using logistic regression.\n",
    "\n",
    "There are a couple of questions we may be left with when we finish this process.\n",
    "\n",
    "* Our model of language is rather unrealistic, but it seems to give us a feature set that predicts Millenialism very well. Some would argue, does the model of language truly matter? I would argue that it actually does. In particular, if we want to make stronger inferential statements about how our language informs a particular outcome variable, we need to have a deeper representation of how our language is encoded beyond word frequencies.\n",
    "\n",
    "* The logistic regression model is only one way of modeling the probability of a binary outcome. Why do we choose just a sigmoidal structure? What if the true conditional probability distribution of $isMillenial$ is given our features ($X$) is slightly different? These become questions that **statistical modeling** and **machine learning** are concerned with.\n",
    "\n",
    "* What is the impact of an imbalanced classes problem on the binary classification task?\n",
    "\n",
    "* How might I be able to incorporate out-of-vocabulary features in my language into my current model?\n",
    "\n",
    "* How might I make my model more flexible to documents and contexts outside of OkCupid Profiles? Is this a worthwhile goal to achieve?\n",
    "\n",
    "For some of these questions, I recommend looking into some of the [Reference Materials](#refMaterials) that I would list below. There are many potential applications of using language data, and today we only worked on a rather simple one. Hopefully, I have given you a decent start on finding ways to manipulate language data on your own. Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"refMaterials\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "605px",
   "left": "0px",
   "right": "auto",
   "top": "106px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
