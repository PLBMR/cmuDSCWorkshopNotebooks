{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tartan Data Science Club : Practical Natural Language Processing\n",
    "\n",
    "_By [Michael Rosenberg](mailto:mmrosenb@andrew.cmu.edu)._\n",
    "\n",
    "_**Description**: This notebook contains an introduction to document analysis with OkCupid data. It is designed to be used at a workshop for introducing individuals to natural language processing._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: What Is Natural Language Processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: WRITE OUT THIS PART**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note for 15-112 Students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: WRITE OUT THIS PART**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"metadataAnalysis\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: WRITE OUT THIS PART**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scanning a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by loading in the dataset of profiles. This is a ```.csv``` file, which stands for Comma-Separated Values. If we take a look at the [text representation of the dataset](data/JSE_OkCupid/profiles.csv), we see that there is a set of column keys in the first row of the ```.csv``` file, and each row below it refers to a filled-in observation of the dataset. In this context, a \"filled-in observation\" is a transcribed OkCupid profile.\n",
    "\n",
    "Typically, we can load in a ```.csv``` file using the ```csv``` package available in base ```Python```. However, for the sake of having a more elegant coding process, I generally use the ```pandas``` package to manipulative large dataframes. You can refer to the [reference materials](#refMaterials) for instructions on how to install ```pandas```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#read in a .csv file\n",
    "okCupidFrame = pd.read_csv(\"data/JSE_OkCupid/profiles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at the dimension of this data frame. This is held in the ```shape``` attribute of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numRows = okCupidFrame.shape[0]\n",
    "numCols = okCupidFrame.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "numCols": "31",
     "numRows": "59946"
    }
   },
   "source": [
    "We see that there are {{numRows}} profie observations in this dataset, which is a sizable amount of profiles to consider. We also see that each profile contains {{numCols}} features, many of which were transcribed by the original data collectors. As discussed in the [metadata analysis](#metadataAnalysis), the language-oriented features are found in the ```essay``` variables. For now, let us consider the self summary variable of the profiles contained in the ```essay0``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selfSummaries = okCupidFrame[\"essay0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first check to see if there are any missing values in this column. This will be important for when we want to use these summaries for predictive purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make conditional on which summaries are empty\n",
    "emptySections = selfSummaries[selfSummaries.isnull()]\n",
    "numNullEntries = emptySections.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "numNullEntries": "5485"
    }
   },
   "source": [
    "We see that we have {{numNullEntries}} profiles without self-summaries. For the sake of considering only completed profiles up to the summary, we will filter out observations with ```NaN``` entries for ```essay0```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get observations with non-null summaries\n",
    "filteredOkCupidFrame = okCupidFrame[okCupidFrame[\"essay0\"].notnull()]\n",
    "#then reobtain self summaries\n",
    "selfSummaries = filteredOkCupidFrame[\"essay0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching and analyzing a single profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basis of natural language processing comes simply from analyzing a string. In this extent, it is natural to start out analysis by analyzing a single document, which in this case is a single self-summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consideredSummary = selfSummaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a string, we can read it by a simple ```print``` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about me:<br />\n",
      "<br />\n",
      "i would love to think that i was some some kind of intellectual:\n",
      "either the dumbest smart guy, or the smartest dumb guy. can't say i\n",
      "can tell the difference. i love to talk about ideas and concepts. i\n",
      "forge odd metaphors instead of reciting cliches. like the\n",
      "simularities between a friend of mine's house and an underwater\n",
      "salt mine. my favorite word is salt by the way (weird choice i\n",
      "know). to me most things in life are better as metaphors. i seek to\n",
      "make myself a little better everyday, in some productively lazy\n",
      "way. got tired of tying my shoes. considered hiring a five year\n",
      "old, but would probably have to tie both of our shoes... decided to\n",
      "only wear leather shoes dress shoes.<br />\n",
      "<br />\n",
      "about you:<br />\n",
      "<br />\n",
      "you love to have really serious, really deep conversations about\n",
      "really silly stuff. you have to be willing to snap me out of a\n",
      "light hearted rant with a kiss. you don't have to be funny, but you\n",
      "have to be able to make me laugh. you should be able to bend spoons\n",
      "with your mind, and telepathically make me smile while i am still\n",
      "at work. you should love life, and be cool with just letting the\n",
      "wind blow. extra points for reading all this and guessing my\n",
      "favorite video game (no hints given yet). and lastly you have a\n",
      "good attention span.\n"
     ]
    }
   ],
   "source": [
    "print consideredSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Figure 1: A self-summary of an individual in our dataset._\n",
    "\n",
    "We can see a couple of things just from looking at this profile:\n",
    "\n",
    "* This man sounds extremely pretentious.\n",
    "\n",
    "* There are some misspellings due to the user-inputted aspects in this self-summary. most notably, the word \"simularities\" should probably be \"similarities.\"\n",
    "\n",
    "* Ther are several ```br``` tags within the document that do not add information to our understanding of the document. These tags are primarily for OkCupid to display the self-summary properly on their website.\n",
    "\n",
    "Thus, before we analyze this dataset, we need to do some data cleansing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and searching with Regular Expression (```regex```)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Regular Expression is defined as a sequence of characters that defines a search pattern. This search pattern is used to \"find\" and \"find and replace\" certain information in strings through string search algorithms. To give an example, say that I am interested in quantifying the narcissism found in the self-summary above. Perhaps I am interested in the number of times that \"i\" shows up in the summary. We represent this with the simple regular expression search query that accounts for the letter $i$ and then accounts for all potential punctuation that usually follows a lone $i$:\n",
    "\n",
    "```i[ \\.,:;?!\\n$]```\n",
    "\n",
    "This expression looks for $i$ and then looks for a potential followup punctuation to indicate that is a lone $i$. This can be a space, period, comma, colon, semi-colon, question mark, explanation point, or an end-of-line marker (```$```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re #regular expression library in base Python\n",
    "#let us compile this for search\n",
    "iRe = re.compile(\"i[ \\.,:?!\\n]\")\n",
    "#then find all the times it occurs in the summary\n",
    "iObservanceList = iRe.findall(consideredSummary)\n",
    "numIs = len(iObservanceList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "numIs": "8"
    }
   },
   "source": [
    "We see that the speaker refers to himself in terms of \"i\" {{numIs}} times in this self-summary. This is actually more reasonable than most people when referring to themselves, but let's try to extend this regular expression to other self-centered terms. We will now search for\n",
    "\n",
    "```(i|me)[ \\.,:?!\\n]```\n",
    "\n",
    "The ```|``` symbol represents an or operator for in a section. In this context, this regular expression is looking for either \"i\" or \"me\" followed by some punctuation in order to identify lone observations of ```i``` and ```me``` instead of appendages on other words (for example, ```i``` in ```intellectual``` and ```me``` in ```meandering```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selfCenteredRe = re.compile(\"(i|me)[ \\.,:?!\\n]\")\n",
    "#find all observations of this regular expression\n",
    "selfObsList = selfCenteredRe.findall(consideredSummary)\n",
    "#get length\n",
    "numNarcissisticWords = len(selfObsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "numNarcissisticWords": "17"
    }
   },
   "source": [
    "We see that when we extend our search to include \"me\" as a possible pattern to recognize, we see that the number of self-referrals increases to {{numNarcissisticWords}}. We can extend this to other aspects of the self-summary, and potentially more interesting patterns we want to find in the language.\n",
    "\n",
    "Regular Expressions can also be used to substitute particular components of the summary for data cleaning purposes. For instance, let us alter the mistake of \"simularities\" as \"similarities\" in the above summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about me:<br />\n",
      "<br />\n",
      "i would love to think that i was some some kind of intellectual:\n",
      "either the dumbest smart guy, or the smartest dumb guy. can't say i\n",
      "can tell the difference. i love to talk about ideas and concepts. i\n",
      "forge odd metaphors instead of reciting cliches. like the\n",
      "similarities between a friend of mine's house and an underwater\n",
      "salt mine. my favorite word is salt by the way (weird choice i\n",
      "know). to me most things in life are better as metaphors. i seek to\n",
      "make myself a little better everyday, in some productively lazy\n",
      "way. got tired of tying my shoes. considered hiring a five year\n",
      "old, but would probably have to tie both of our shoes... decided to\n",
      "only wear leather shoes dress shoes.<br />\n",
      "<br />\n",
      "about you:<br />\n",
      "<br />\n",
      "you love to have really serious, really deep conversations about\n",
      "really silly stuff. you have to be willing to snap me out of a\n",
      "light hearted rant with a kiss. you don't have to be funny, but you\n",
      "have to be able to make me laugh. you should be able to bend spoons\n",
      "with your mind, and telepathically make me smile while i am still\n",
      "at work. you should love life, and be cool with just letting the\n",
      "wind blow. extra points for reading all this and guessing my\n",
      "favorite video game (no hints given yet). and lastly you have a\n",
      "good attention span.\n"
     ]
    }
   ],
   "source": [
    "#make the re\n",
    "simRe = re.compile(\"simularities\")\n",
    "#then perform a sub\n",
    "filteredSummary = simRe.sub(\"similarities\",consideredSummary)\n",
    "print filteredSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Figure 2: The filtered summary after changing the stated spelling issue._\n",
    "\n",
    "As we can see, \"simularities\" was changed to \"similarities\" without us having to find the exact beginning and ending indices for the \"simularities\" mistake. We can continue this cleaning by altering an even larger interpretation issue: the ```br``` tags. These tags are primarily used for OkCupid to understand how to display the text, but they generally are not informative to the summary itself.\n",
    "\n",
    "We will remove these by building the regular expression\n",
    "\n",
    "```<.*>```\n",
    "\n",
    "The ```.``` is meant to represent any character available in the ASCII encoding framework. the ```*``` is meant to represent \"0 or more observations of the prior character or expression.\" In this case, this regular expression is asking to find strings that start with \"<\" and end with \">\" and feature any number of characters in between \"<\" and \">.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about me:\n",
      "\n",
      "i would love to think that i was some some kind of intellectual:\n",
      "either the dumbest smart guy, or the smartest dumb guy. can't say i\n",
      "can tell the difference. i love to talk about ideas and concepts. i\n",
      "forge odd metaphors instead of reciting cliches. like the\n",
      "similarities between a friend of mine's house and an underwater\n",
      "salt mine. my favorite word is salt by the way (weird choice i\n",
      "know). to me most things in life are better as metaphors. i seek to\n",
      "make myself a little better everyday, in some productively lazy\n",
      "way. got tired of tying my shoes. considered hiring a five year\n",
      "old, but would probably have to tie both of our shoes... decided to\n",
      "only wear leather shoes dress shoes.\n",
      "\n",
      "about you:\n",
      "\n",
      "you love to have really serious, really deep conversations about\n",
      "really silly stuff. you have to be willing to snap me out of a\n",
      "light hearted rant with a kiss. you don't have to be funny, but you\n",
      "have to be able to make me laugh. you should be able to bend spoons\n",
      "with your mind, and telepathically make me smile while i am still\n",
      "at work. you should love life, and be cool with just letting the\n",
      "wind blow. extra points for reading all this and guessing my\n",
      "favorite video game (no hints given yet). and lastly you have a\n",
      "good attention span.\n"
     ]
    }
   ],
   "source": [
    "tagRe = re.compile(\"<.*>\")\n",
    "filteredSummary = tagRe.sub(\"\",filteredSummary)\n",
    "print filteredSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Figure 3: Our filtered summary after all ```br``` tags have been removed._\n",
    "\n",
    "As we can see, we have cleaned the summary to a point where there are no tags whatsoever in the text. We can then use this edited summary within the main dataset. This process is essentially a form of data cleansing with text.\n",
    "\n",
    "If you would like to learn more about ```regex```, see the links in the [reference materials](#refMaterials)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Analysis on Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we are interested in the terms and expressions used by a person in a document. The atom of a document to some degree is a word, and so it seems appropriate to start analyzing the types of words used in this document.\n",
    "\n",
    "We will first do a \"tokenization\" of this document. Tokenization is when you simplify a document to just the sequence of words that make up the document. This requires us to split our document into a list based on certain punctuation marks (such as a ```.```, a new-line character, or a ```,```) and spaces, and then filtering our list into non-degenerate words (i.e. not the null string \"\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make our split\n",
    "sumWordList = re.split(\" |\\(|\\)|\\.|\\n|,|:|;\",filteredSummary)\n",
    "#filter to have non-degenerate words\n",
    "filSumWordList = []\n",
    "for word in sumWordList:\n",
    "    if (len(word) > 0): filSumWordList.append(word)\n",
    "#this gave us number of words, let us get number of unique words\n",
    "numUniqueWords = len(set(filSumWordList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "len(filSumWordList)": "234",
     "numUniqueWords": "144"
    }
   },
   "source": [
    "We see that {{len(filSumWordList)}} non-unique words occur in this document. These are referred to as **tokens** in NLP. Of those non-unique words, we see that {{numUniqueWords}} unique words occur in this document. These distinct words are referred to as **types** in NLP. The relationship between tokens and types form the shallow basis of complexity in sentences.\n",
    "\n",
    "In this context, since the ratio of types to tokens is around .61, there is a surprising amount of diversity in vocabulary used in this document. Let us see what are the most common words used in this document. We will key words by count in the dictionary and then pick out the words found in the highest keys to get our most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordCountDict = {}\n",
    "for word in set(filSumWordList): #look through unique words\n",
    "    numOccurences = 0\n",
    "    for token in filSumWordList:\n",
    "        if (word == token): #that means we have seen an occurences\n",
    "            numOccurences += 1\n",
    "    #after running through our word list, let us add it to the dictionary keyed\n",
    "    #by count\n",
    "    if (numOccurences in wordCountDict): #already have the list started\n",
    "        wordCountDict[numOccurences].append(word)\n",
    "    else: #start the list\n",
    "        wordCountDict[numOccurences] = [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : ['all', 'just', \"don't\", 'telepathically', 'hiring', 'still', 'yet', 'letting', 'underwater', 'similarities', 'only', 'willing', 'snap', 'smile', 'smart', 'good', 'conversations', 'kind', 'choice', 'game', 'five', 'know', 'kiss', 'instead', 'difference', 'cool', 'like', 'leather', 'stuff', 'either', 'guessing', 'old', 'tired', 'house', 'decided', 'dumb', 'are', 'year', 'our', 'odd', 'for', 'spoons', 'between', 'got', 'reading', 'lazy', 'attention', 'given', 'smartest', 'wear', 'dress', 'by', 'both', 'weird', 'think', 'extra', 'intellectual', 'rant', 'laugh', 'dumbest', 'lastly', 'your', 'little', 'span', 'blow', 'was', 'tell', 'friend', 'that', 'am', 'tying', 'probably', 'hints', 'myself', 'word', 'this', 'work', 'as', 'considered', 'while', 'silly', 'can', 'concepts', 'video', 'at', \"mine's\", 'is', 'mind', 'mine', 'deep', 'an', 'say', 'bend', 'hearted', 'seek', 'everyday', 'funny', 'no', 'cliches', 'things', 'ideas', 'productively', 'forge', 'tie', 'out', 'reciting', 'most', 'wind', 'light', 'or', 'points', \"can't\", 'serious', 'talk']\n",
      "2 : ['should', 'better', 'guy', 'favorite', 'metaphors', 'would', 'way', 'life', 'but', 'in', 'able', 'salt']\n",
      "3 : ['some', 'really', 'with', 'my', 'make']\n",
      "4 : ['shoes', 'about', 'love']\n",
      "5 : ['be', 'me']\n",
      "6 : ['of', 'and', 'have', 'a', 'the']\n",
      "8 : ['you', 'i']\n",
      "13 : ['to']\n"
     ]
    }
   ],
   "source": [
    "#print this out by key\n",
    "for count in wordCountDict:\n",
    "    print count, \":\", wordCountDict[count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_Figure 4: Our words keyed by their frequency in the document._\n",
    "\n",
    "We see that personal references to \"you\" and \"i\" seem to show up frequenty, and words such as \"love\" and \"me\" also have a sizable number of occurences. However, we do notice that there are a lot of words that show up generally infrequently, and only a small number of words that repeat themselves at the very least. We will see this behavior in our word distribution occur in our main corpus in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics on a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then extend our analysis from the single document level to a multi-document level. In NLP, we refer to a collection of documents as a **corpus**. If you study this practice further, you will see that a collection of collections of documents is referred to as **corpora**.\n",
    "\n",
    "We will start our macro-analysis by simply counting the words that occur in all self-summaries and studying the commonality of certain words in these OkCupid profiles.\n",
    "\n",
    "We will import several packages that may be unfamiliar. ```nltk``` is the [Natural Language Toolkit](http://www.nltk.org), which is perhaps the most important package in modern programming for language analysis and document processing. ```collections``` allows us to use data structures that not only provides us with a dictionary-like object, but also keeps track of some measure of order in our dictionary; in our case, it allows us to keep track of the most frequent words in our corpus. ``StringIO`` gives us a flexible mechanism to write the entire corpus (all self-summaries) out into one readable string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imports discussed\n",
    "import nltk #for relevant corpora\n",
    "import collections as co #for ordered dictionary\n",
    "import StringIO #for string manipulation\n",
    "#create a writeable string object\n",
    "stringWriteTerm = StringIO.StringIO()\n",
    "#write all summaries to the string write term\n",
    "filteredOkCupidFrame[\"essay0\"].apply(lambda x: stringWriteTerm.write(x))\n",
    "#get the full string from the writer\n",
    "summaryString = stringWriteTerm.getvalue()\n",
    "stringWriteTerm.close()\n",
    "#lower the letering and split into list of words\n",
    "summaryString = summaryString.lower()\n",
    "summaryWordList = re.split(\"\\.| |,|;|-|\\n\",summaryString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a string of the entire ordered corpus, we will now filter to only include legal words in the English dictionary and remove any potential stopwords. Stopwords are words that do not indicate significance in understanding the contents of a sentence. These are typically words like \"the\", \"to\", \"on.\" For an understanding of how language practitioners choose these stopwords, please see the [reference materials](#refMaterials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get legal words and get stop words\n",
    "legalWordSet = set(nltk.corpus.words.words())\n",
    "stopWordSet = set(nltk.corpus.stopwords.words())\n",
    "#then make our filtration\n",
    "filteredSumWordList = [word for word in summaryWordList\n",
    "                           if word in legalWordSet and\n",
    "                              word not in stopWordSet]\n",
    "#then count the frequency of words in a collection\n",
    "filteredWordCounter = co.Counter(filteredSumWordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"refMaterials\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "605px",
   "left": "0px",
   "right": "auto",
   "top": "106px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
